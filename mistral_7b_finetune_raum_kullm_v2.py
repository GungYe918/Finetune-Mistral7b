# -*- coding: utf-8 -*-
"""Mistral-7B-finetune-RAUM-kullm-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0MH1pB1pAYz1KcBQ814Ty8Ebmd30b0_
"""

!pip install -q autotrain-advanced==0.6.51

!autotrain setup --update-torch

!pip install --upgrade tensorflow

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!autotrain llm --train \
    --project_name "Mistral-7B-finetune-RAUM_kullm-v2-part4" \
    --model GungYe/Mistral-7B-finetune-RAUM-kullm_v2-part3 \
    --data_path "GungYe/kullm-v2-part4" \
    --text_column "text" \
    --use_peft \
    --use_int4 \
    --learning_rate 2e-4 \
    --train_batch_size 16 \
    --num_train_epochs 4 \
    --trainer sft \
    --model_max_length 2048 \
    --target_modules q_proj,v_proj \
    --merge-adapter \
    --token "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" \
    --push_to_hub \
    --repo_id GungYe/Mistral-7B-finetune-RAUM-kullm_v2-part4

"""# Save Model"""

from google.colab import drive
drive.mount("/content/drive")

!cp -a /content/Mistral-7B-finetune-RAUM /content/drive/MyDrive

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import HfApi
api = HfApi()

api.upload_folder(
    folder_path="/content/Mistral-7B-finetune-RAUM_kullm-v2-part4",
    repo_id="GungYe/Mistral-7B-finetune-RAUM",
    repo_type="model",
)

"""# Play model"""

!pip install -q transformers peft bitsandbytes

from huggingface_hub import notebook_login
notebook_login()

"""# Huggingface 에서 불러오기"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "GungYe/Mistral-7B-finetune-RAUM-kullm_v2-part3"

bnb_config = BitsAndBytesConfig(
    load_in_8bit=False,
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=None,
    llm_int8_enable_fp32_cpu_offload=False,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
    bnb_4bit_compute_dtype="float16",
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"": 0})
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.eval()

"""#### Local 저장소에서 불러오기"""

### Local 저장소에서 모델 불러오기

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# 로컬 저장소의 경로로 변경.
local_model_path = "/content/Mistral-7B-finetune-RAUM_kullm-v2-part4"

bnb_config = BitsAndBytesConfig(
    load_in_8bit=False,
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=None,
    llm_int8_enable_fp32_cpu_offload=False,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
    bnb_4bit_compute_dtype="float16",
)

# 로컬 경로를 사용하여 모델과 토크나이저 불러오기
model = AutoModelForCausalLM.from_pretrained(local_model_path, quantization_config=bnb_config, device_map={"": 0})
tokenizer = AutoTokenizer.from_pretrained(local_model_path)

model.eval()

prompt = "You are RAUM, an artificial intelligence assistant. You can programming, translate documentation, and provide brief information. You use both English and Korean. Write a response that appropriately completes the request. Below is an instruction that describes a task. ### Instruction: %s ### Response: "

def gen(x):
    q = prompt % (x,)
    gened = model.generate(
        **tokenizer(
            q,
            return_tensors='pt',
            return_token_type_ids=False
        ).to('cuda'),
        max_new_tokens=800,
        early_stopping=True,
        do_sample=True,
    )
    return tokenizer.decode(gened[0]).replace(q, "")

gen("다음의 기업에 대해 알려줘. ### Input: Microsoft")

gen("너는 누구야?")

gen("너는 어떤 일을 할 수 있어?")

gen("다음의 수를 오름차순으로 정렬하는 c언어 코드를 작성해줘 2, 5, 1, 7, 3")

gen("사과는 무슨 맛이야?")

gen("Merry Christmas!")

gen("일론 머스크가 보유한 기업에 대해 알려줘, answer in English")

gen("Describe a company called google, and answer in English.")

gen("C언어로 1+7의 값을 출력하는 코드를 작성해줘")

gen("C언어로 4 * 9의 값을 출력하는 코드를 작성해줘")

gen("코로나 19에 걸렸을 때 어떻게 해야해?")